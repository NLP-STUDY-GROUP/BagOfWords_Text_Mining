{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. 카운트 기반의 문서표현\n",
    "* BOW(Bag of Words)\n",
    "    * 단어별 카운트를 기반으로 문서로부터 특성을 추출하고 표현하는 방식 \n",
    "    * 문서에서 단어의 사용 여부만 표시하는 방법 \n",
    "    * 단어 수를 세어 표시하는 방법 \n",
    "    * 단어가 문서에 나타난 수를 반영해 보정하는 방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 카운트 기반 문서 표현의 개념 \n",
    "* 문서의 의미를 반영해 벡터를 만드는 과정\n",
    "* 문서에 나타난 단어의 통계를 이용해 문서의 내용을 이해하고자 하는 시도 \n",
    "* 단어의 빈도 파악, 어떤 단어가 주로 사용됐는지를 파악하여 내용을 유추함 \n",
    "\n",
    "* 텍스트 마이닝 \n",
    "    * 대상 텍스트를 우리가 다룰 수 있는 수치 형태로 변환해야 함 \n",
    "    * 이때, 각 수치는 그 텍스트의 특성(feature)를 표현함 \n",
    "    * 텍스트의 특성을 정의하고 그 값을 텍스트로 구분함 \n",
    "\n",
    "* 텍스트의 특성 \n",
    "    * 텍스트의 특성 : 단어 \n",
    "    * 특성이 갖는 값 : 단어가 텍스트에서 나타난 횟수 \n",
    "\n",
    "* 문제점 1 : 각 문서별 다른 특성 \n",
    "    * 모든 문서의 특성이 서로 같아야 비교가 가능 (문서의 특성 제각각)\n",
    "    * 비교 가능하도록 동일한 문서 특성을 갖게 하기 위해 : 동일한 단어들로 특성 표현 \n",
    "    * 말뭉치에 대해 단어 집합(vocabulary) 구성\n",
    "    * 단어 집합을 대상으로 각 문서에 대해 빈도를 표시 \n",
    "        * 말뭉치(corpus) : 언어연구를 위해 텍스트를 가공,처리,분석할 수 있는 자료의 집합 \n",
    "\n",
    "* 문제점 2: 희소 벡터(sparse vector)\n",
    "    * 사용하지 않은 단어들이 훨씬 많이 포함됨 \n",
    "    * 문서를 표현하기 위해 너무 많은 특성을 사용, 그 특성 중 극히 일부만 값을 갖음 \n",
    "    * 대부분 값이 0인 특성 벡터: 희소 벡터(sparse vector)\n",
    "    * 저장 공간, 연산 측면에서 비효율적 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. 텍스트는 우리가 정의한 특성에 대한 특성 값의 집합(벡터) 로 변환함 \n",
    "* 2. 단어: 특성/ 단어의 빈도 : 특성값 -> 이러한 방식 BOW(BAG of Words) -> 순서가 사라짐 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 BOW 기반의 카운트 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\aa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------\n",
    "# bow 실습, nltk에서 제공하는 영화 리뷰 사용 \n",
    "# movie_reviews = 2,000여개 영화 리뷰, 그 리뷰의 내용에 대한 감성 label(긍정, 부정)\n",
    "# https://www.nltk.org/api/nltk.sentiment.util.html?highlight=movie_reviews#nltk.sentiment.util.demo_movie_reviews\n",
    "\n",
    "# fileids()\n",
    "    # 영화 리뷰 문서들의 id(fileid)를 반환 \n",
    "    # 매개변수 categories 를 이용하면 특정 분류에 속하는 문서들의 id 만 가져올 수 있음 \n",
    "\n",
    "# categories()\n",
    "    # 리뷰 문서들에 대한 분류, 라벨값 \n",
    "    # 감성을 표현하는 긍정('pos') & 부정('neg')\n",
    "\n",
    "# raw()\n",
    "    # 리뷰 문서의 원문을 문자열의 리스트 형태로 봔한 \n",
    "    # 인수로 fileids 를 주면 특정 문서만 가져올 수 있음 \n",
    "\n",
    "# sents()\n",
    "    # 리뷰 문서의 원문에 대해 nltk 의 sent_tokenize 로 토큰화한 문장들을 \n",
    "    # 다시 word_tokenize로 토큰화한 결과를 반환 \n",
    "    # 인수로 fileid를 주면 특정 문서에 대한 토큰화 결과를 가져옴 \n",
    "\n",
    "# words()\n",
    "    # 리뷰 문서의 원문에 대한 NLTK의 word_tokenize로 토큰화한 결과를 반환 \n",
    "    # 인수로 fileid 를 주며 특정 문서에 대한 토큰화 결과를 가져옴 \n",
    "#----------------------------------------\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#review count: 2000\n",
      "#samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
      "#categories of reviews: ['neg', 'pos']\n",
      "#num of \"neg\" reviews: 1000\n",
      "#num of \"pos\" reviews: 1000\n",
      "#id of the first review: neg/cv000_29416.txt\n",
      "#first review content:\n",
      " plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "w\n",
      "\n",
      "#sentence tokenization result: [['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.']]\n",
      "#word tokenization result: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print('#review count:', len(movie_reviews.fileids())) #영화 리뷰 문서의 id를 반환\n",
    "print('#samples of file ids:', movie_reviews.fileids()[:10]) #id를 10개까지만 출력\n",
    "print('#categories of reviews:', movie_reviews.categories()) # label, 즉 긍정인지 부정인지에 대한 분류\n",
    "print('#num of \"neg\" reviews:', len(movie_reviews.fileids(categories='neg'))) #label이 부정인 문서들의 id를 반환\n",
    "print('#num of \"pos\" reviews:', len(movie_reviews.fileids(categories='pos'))) #label이 긍정인 문서들의 id를 반환\n",
    "fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환\n",
    "print('#id of the first review:', fileid)\n",
    "print('#first review content:\\n', movie_reviews.raw(fileid)[:200]) #첫번째 문서의 내용을 200자까지만 출력\n",
    "print()\n",
    "print('#sentence tokenization result:', movie_reviews.sents(fileid)[:2]) #첫번째 문서를 sentence tokenize한 결과 중 앞 두 문장\n",
    "print('#word tokenization result:', movie_reviews.words(fileid)[:20]) #첫번째 문서를 word tokenize한 결과 중 앞 스무 단어\n",
    "\n",
    "\n",
    "# 총 2,000개의 리뷰 문서\n",
    "# fileid는 문자열로 되어 있고, 감성과 파일명이 포함됨 - ex. 'neg/cv000_29416.txt'\n",
    "# movie_reviews.sents() - 문서를 문장 단위로 토큰화, 문장을 단어 단위로 토큰화 - 리스트의 리스트 \n",
    "# movie_reviews.words() - 원문 전체에 대해 바로 단어 단위로 토큰화, 결과가 하나의 리스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "# BOW - 텍스트를 특성 벡터로 변환 , 특성 값 : 빈도수 \n",
    "#----------------------------------------\n",
    "# 문서 집합으로부터 특성 벡터를 추출하는 과정 \n",
    "\n",
    "# 1. 텍스트 전처리 -> 의미가 있는 최소 단위의 리스트로 변환 \n",
    "    # 토큰화, 정규화, 품사 태깅 등의 방법 사용 \n",
    "    # ex. movie_reviews.words() 가 제공하는 결과 사용 \n",
    "\n",
    "# 2. 특성 추출 대상이 되는 단어 집합을 구성 \n",
    "    # 특성 집합 : 어휘 집합 \n",
    "    # 말뭉치에 있는 모든 단어를 사용하기 보단 자신이 정한 기준에 따라 단어 선별 \n",
    "    # ex. 단어에 대한 빈도를 계산하고, 빈도가 높은 상위 단어 n개만 사용 \n",
    "\n",
    "# 3. 각 문서별로 특성 추출 대상 단어들에 대해 단어의 빈도를 계산- > 특성 벡터 추출 \n",
    "    # 단어의 빈도를 특성 값으로 사용 : 카운트 벡터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch']\n"
     ]
    }
   ],
   "source": [
    "# 1. 각 문서에 대한 토큰화 결과들로 리스트를 만듦 \n",
    "# fileids() 를 이용해 몯느 문서의 id를 가져오고\n",
    "# 각 id 들에 대한 words()로 토큰화 결과를 가져와 리스트를 만듦\n",
    "\n",
    "documents = [list(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()]\n",
    "print(documents[0][:50]) # 첫째 문서의 앞 50 개 단어 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of ',': 77717, count of 'the': 76529, count of '.': 65876, count of 'a': 38106, count of 'and': 35576, count of 'of': 34123, count of 'to': 31937, count of ''': 30585, count of 'is': 25195, count of 'in': 21822, "
     ]
    }
   ],
   "source": [
    "# 딕셔너리로 단어별 말뭉치 전체에서의 빈도를 계산 \n",
    "# 빈도가 높은 단어부터 정렬하여 빈도수 상위 20 개 단어를 출력 \n",
    "\n",
    "word_count = {}\n",
    "for text in documents:\n",
    "    for word in text:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
    "for word in sorted_features[:10]:\n",
    "    print(f\"count of '{word}': {word_count[word]}\", end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of features: 43030\n",
      "count of 'film': 8935, count of 'one': 5791, count of 'movie': 5538, count of 'like': 3690, count of 'even': 2564, count of 'time': 2409, count of 'good': 2407, count of 'story': 2136, count of 'would': 2084, count of 'much': 2049, "
     ]
    }
   ],
   "source": [
    "# ' '.'the','a' 와 같이 의미적으로 쓸모 없는 단어의 빈도가 높음 \n",
    "# 정규표현식 통해 다시 토큰화 \n",
    "# raw() 를 이용해 원문을 가져와 documents 를 만들고 \n",
    "# 이에 대해 토큰화 \n",
    "# NLTK 가 제공하는 불용어 사전을 이용해 불용어를 제거 \n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords #일반적으로 분석대상이 아닌 단어들\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\") # 정규포현식으로 토크나이저를 정의\n",
    "english_stops = set(stopwords.words('english')) #영어 불용어를 가져옴\n",
    "\n",
    "#words() 대신 raw()를 이용해 원문을 가져옴\n",
    "documents = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()] \n",
    "# print('documents', documents[0])\n",
    "\n",
    "# stopwords의 적용과 토큰화를 동시에 수행.\n",
    "tokens = [[token for token in tokenizer.tokenize(doc) if token not in english_stops] for doc in documents]\n",
    "# print('documents', tokens[0:4])\n",
    "\n",
    "word_count = {}\n",
    "for text in tokens:\n",
    "    for word in text:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
    "\n",
    "print('num of features:', len(sorted_features))\n",
    "for word in sorted_features[:10]:\n",
    "    print(f\"count of '{word}': {word_count[word]}\", end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'one', 'movie', 'like', 'even', 'time', 'good', 'story', 'would', 'much'],"
     ]
    }
   ],
   "source": [
    "# 단어로 구성된 특성 집합 \n",
    "# 상위 빈도수를 가지는 단어 천 개만 추출해서 최종적으로 문서를 표현할 특성으로 사용하기로 함 \n",
    "# 빈도가 높은 상위 1000개의 단어만 추출하여 features를 구성\n",
    "word_features = sorted_features[:1000] \n",
    "print(word_features[0:10], end= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# 주어진 문서를 특성 벡터, 즉, 카운트 벡터로 변환하는 함수 \n",
    "# 특성 집합(word_feature) : 단어들의 빈도에 따라 순서가 정해져 있음 \n",
    "# 카운트 벡터: word_feature 단어의 순서에 따라 단어의 빈도 기록 \n",
    "    # 특성 집합 예제 (word_features_ex) : ['one','two','teen','couples','solo']\n",
    "    # 주어진 문서의 토큰화 결과: ['two','two','couples']\n",
    "    # 변한된 카운트 벡터 결과: [0,2,0,1,0]\n",
    "\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    word_count = {}\n",
    "    for word in document: #document에 있는 단어들에 대해 빈도수를 먼저 계산\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "        \n",
    "    features = []\n",
    "    for word in word_features: #word_features의 단어에 대해 계산된 빈도수를 feature에 추가\n",
    "        features.append(word_count.get(word, 0)) #빈도가 없는 단어는 0을 입력\n",
    "    return features\n",
    "\n",
    "word_features_ex = ['one', 'two', 'teen', 'couples', 'solo']\n",
    "doc_ex = ['two', 'two', 'couples']\n",
    "print(document_features(doc_ex, word_features_ex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(film, 5), (one, 3), (movie, 6), (like, 3), (even, 3), (time, 0), (good, 2), (story, 0), (would, 1), (much, 0), (also, 1), (get, 3), (character, 1), (two, 2), (well, 1), (first, 0), (characters, 1), (see, 2), (way, 3), (make, 5), "
     ]
    }
   ],
   "source": [
    "# 전체 리뷰 집합에 대해 적용\n",
    "# word_features 의 단어는 빈도가 높은 순서대로 정렬 \n",
    "feature_sets = [document_features(d, word_features) for d in tokens]\n",
    "\n",
    "# 첫째 feature set의 내용을 앞 20개만 word_features의 단어와 함께 출력\n",
    "for i in range(20):\n",
    "    print(f'({word_features[i]}, {feature_sets[0][i]})', end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 뒤로 갈수록 카운트 값이 0이 아닌 단어가 많이 포함 \n",
    "# 마지막 20개 값을 출력 \n",
    "print(feature_sets[0][-20:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 사이킷런을 이용한 카운트 벡터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "* scikit-learn (sklearn)\n",
    "* sklearn.feature_extraction 모듈 \n",
    "* CountVectorizer 클래스 \n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "---\n",
    "* 매개변수 \n",
    "    * tokenizer\n",
    "        * 함수 형태로 외부 토크나이저 저장\n",
    "        * 지정하지 않으면 자체 토크나이저를 사용 \n",
    "    * stop_words \n",
    "        * 리스트 형태로 불용어 사전을 지정 \n",
    "        * 'english' 로 값을 주면 자체 영어 불용어 사전을 사용 \n",
    "    * ngram_range\n",
    "        * (min_n, max_n) 의 튜플 형태로 ngram 범위 지정 \n",
    "        * 기본값 (1,1)\n",
    "    * max_df \n",
    "        * 단어로 특성 구성할 때, \n",
    "        * 문서에 나타난 빈도(document frequency)가 max_df 보다 크면 제외함 \n",
    "        * 비율/ 문서의 수로 지정 가능 \n",
    "    * min_df \n",
    "        * 단어로 특성을 구성할 때, \n",
    "        * 문서에 나타난 빈도(document frequency)가 min_df 보다 작으면 제외함 \n",
    "        * 비율/ 문서의 수로 지정 가능 \n",
    "    * max_features\n",
    "        * 최대 특성의 수 지정 \n",
    "        * 지정하지 않으면 전체 단어를 사용함 \n",
    "    * vocabulary   \n",
    "        * 특성으로 사용할 단어들을 직접 지정함 \n",
    "    * binary \n",
    "        * True 값을 주면 빈도 대신 1절에서 배운 단어의 유무(1/0) 로 특성 값을 생성 \n",
    "\n",
    "---  \n",
    "* 메서드 \n",
    "    * fit(raw_documents) \n",
    "        * tokenizer, stop_words, max_df, min_df, max_features 등을 이용해\n",
    "         문서 집합 전체에 대해 토큰화, 불용어 제거, 특성 선택 수행하여 특성 집합을 생성 \n",
    "        * 인수로 주어진 문서 집합(raw_documents)에 대한 토큰화 수행 \n",
    "        * 특성 집합 생성 (단어, 인덱스)\n",
    "            * (0,0): 5 : ((0: 첫 문자, 0 : 특성 집합의 인덱스, 5: 이 특성의 빈도)\n",
    "            * 즉, 첫 문서에서 'film' 은 5회 나타남 \n",
    "    * transform(raw_documents)\n",
    "        * fit() 에서 생성한 특성 집합 이용해 \n",
    "          인수로 주어진 문서 집합 (raw_documents)에 대한 카운트벡터로 변환해 반환 \n",
    "    * fit_transform(raw_documents)\n",
    "        * 인수로 주어진 문서 집합(raw_documents)에 대해 fit, transform 동시에 수행 \n",
    "    * get_feature_names_out()\n",
    "        * 특성 집합에 있는 특성의 이름 \n",
    "        * 즉, 단어를 순서대로 반환 \n",
    "        * sklearn 1.0 으로 바뀌기 전에 get_feature_names() \n",
    "        * sklearn 의 버젼 확인하고 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 준비, movie_reviews.raw()를 사용하여 raw text를 추출\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "# print(movie_reviews.fileids()[0:3]) # 'neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt'\n",
    "# print(movie_reviews.raw('neg/cv000_29416.txt')) # raw_documents, 리뷰 원문 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_features(0~4):  ['film', 'one', 'movie', 'like', 'even']\n",
      "CountVectorizer(vocabulary=['film', 'one', 'movie', 'like', 'even', 'time',\n",
      "                            'good', 'story', 'would', 'much', 'also', 'get',\n",
      "                            'character', 'two', 'well', 'first', 'characters',\n",
      "                            'see', 'way', 'make', 'life', 'really', 'films',\n",
      "                            'plot', 'little', 'people', 'could', 'bad', 'scene',\n",
      "                            'never', ...])\n"
     ]
    }
   ],
   "source": [
    "# vocabulary 매개변수 : 앞에서 만든 word_features 에 있는 단어들만으로 벡터 구성 \n",
    "# 바로 분석을 시작할 때 max_features 에 빈도 높은 단어부터 사용할 단어 지정할 수 있음 \n",
    "# CounterVectorizer 객체 생성 \n",
    "\n",
    "print('word_features(0~4): ',word_features[0:5])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#cv = CountVectorizer() #모든 매개변수에 디폴트 값을 사용하는 경우\n",
    "\n",
    "#앞에서 생성한 word_features를 이용하여 특성 집합을 지정하는 경우\n",
    "cv = CountVectorizer(vocabulary=word_features) \n",
    "\n",
    "#cv = CountVectorizer(max_features=1000) #특성 집합을 지정하지 않고 최대 특성의 수를 지정하는 경우\n",
    "print(cv) #객체에 사용된 인수들을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film' 'one' 'movie' 'like' 'even' 'time' 'good' 'story' 'would' 'much'\n",
      " 'also' 'get' 'character' 'two' 'well' 'first' 'characters' 'see' 'way'\n",
      " 'make']\n",
      "['film', 'one', 'movie', 'like', 'even', 'time', 'good', 'story', 'would', 'much', 'also', 'get', 'character', 'two', 'well', 'first', 'characters', 'see', 'way', 'make']\n"
     ]
    }
   ],
   "source": [
    "# fit_transform() 으로 특성 집합 생성, 카운트 벡터 생성 \n",
    "# get_feature_names_out() : word_features와 사용된 단어 및 순서 동일 \n",
    "\n",
    "reviews_cv = cv.fit_transform(reviews) #reviews를 이용하여 count vector를 학습하고, 변환\n",
    "print(cv.get_feature_names_out()[:20]) # count vector에 사용된 feature 이름을 반환\n",
    "print(word_features[:20]) # 비교를 위해 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#type of count vectors: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "#shape of count vectors: (2000, 1000)\n",
      "#sample of count vector:\n",
      "  (0, 0)\t6\n",
      "  (0, 1)\t3\n",
      "  (0, 2)\t6\n",
      "  (0, 3)\t3\n",
      "  (0, 4)\t3\n",
      "  (0, 6)\t2\n",
      "  (0, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "# reviews_cv(리뷰 문서에 대한 카운트 벡터)\n",
    "# 타입: scipy.sparse.csr.csr_matrix \n",
    "# shape (2000,1000) : 리뷰의 수 2000개, 각 리뷰마다 특성의 수가 1,000개 \n",
    "\n",
    "print('#type of count vectors:', type(reviews_cv))\n",
    "print('#shape of count vectors:', reviews_cv.shape)\n",
    "print('#sample of count vector:')\n",
    "print(reviews_cv[0, :10])\n",
    "\n",
    "# (0,0), (0,1) : 좌표 , 오른쪽 숫자: 빈도수 \n",
    "# 빈도가 있는 것만 저장 (0 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 252984 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2000 * 1000 크기\n",
    "# 실제 인자 252,984 , 나머지 값은 0 \n",
    "# 252,984 /(2,000*1,000), 즉 12.65% 만 인자 있음 , 희소 행렬 \n",
    "# Compressed Sparse Row format 데이터 타입 사용 \n",
    "reviews_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 6, 3, 3, 0, 2, 0, 1, 0, 1, 3, 1, 2, 1, 0, 1, 2, 3, 5]\n",
      "[6 3 6 3 3 0 2 0 1 0 1 3 2 2 1 0 1 2 3 5]\n"
     ]
    }
   ],
   "source": [
    "print(feature_sets[0][:20]) #절 앞에서 직접 계산한 카운트 벡터\n",
    "print(reviews_cv.toarray()[0, :20]) #변환된 결과의 첫째 feature set 중에서 앞 20개를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film:6\n",
      "one:3\n",
      "movie:6\n",
      "like:3\n",
      "even:3\n"
     ]
    }
   ],
   "source": [
    "# get_feature_names_out () : 반환하는 특성\n",
    "# 단어들의 순서: review_cv 에 있는 특성 값들의 순서 일치 \n",
    "for word, count in zip(cv.get_feature_names_out()[:20], reviews_cv[0].toarray()[0, :5]):\n",
    "    print(f'{word}:{count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTM & TDM\n",
    "    # DTM (Document Term Matrix) \n",
    "        # 문서를 행, 단어를 열로 해서 단어의 빈도를 나타낸 행렬 \n",
    "        # CountVectorizer 로 생성한 reviews_cv 행렬 \n",
    "        # reviews_cv (2000,1000): 리뷰 수 , 특성의 수 \n",
    "    \n",
    "    # TDM(Term Document Matrix)\n",
    "        # DTM 의 전치 행렬 \n",
    "        # 행과 열을 바꾼 것, 대각선을 축으로 반사시킨 결과  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 한국어 텍스트의 카운트 벡터 변환\n",
    "* CountVectorizer 가 제공하는 기본 토크나이저를 쓸 수 없음 \n",
    "* KoNLPy의 형태소 분석기를 지정, CountVectorizer 객체를 선언 \n",
    "\n",
    "* 한국어 공개 데이터 \n",
    "    * 네이버 영화 리뷰, 텍스트 파일로 제공 \n",
    "        * 감성 분석 \n",
    "    * 네이버 뉴스 분류, NNST 파이썬 라이브러리 형태로 제공 \n",
    "        * 정치, 경제 등 장르 분류 \n",
    "    * 네이버 뉴스 분류, 데이터 파일로 제공  \n",
    "    \n",
    "\n",
    "* 예제 다음(Daum) 영화 리뷰 크롤링해 만든 데이터 \n",
    "    * 리뷰 내용, 평점, 날짜, 영화 제목으로 이루어짐 \n",
    "    * 평점, 영화 제목- 감성 분석 , 영화 제목 예측, 분류 가능 \n",
    "    * 날짜를 통한 토픽 트렌드 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>나는 재밌게 봄</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.14</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5점은 줄 수 없냐?</td>\n",
       "      <td>0</td>\n",
       "      <td>2018.10.10</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>헐..다 죽었어....나중에 앤트맨 보다가도 깜놀...</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.08</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>충격 결말</td>\n",
       "      <td>9</td>\n",
       "      <td>2018.10.06</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>응집력</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.05</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "5                                           나는 재밌게 봄      10  2018.10.14   \n",
       "6                                      0.5점은 줄 수 없냐?       0  2018.10.10   \n",
       "7                     헐..다 죽었어....나중에 앤트맨 보다가도 깜놀...      10  2018.10.08   \n",
       "8                                              충격 결말       9  2018.10.06   \n",
       "9                                                응집력       8  2018.10.05   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  \n",
       "5  인피니티 워  \n",
       "6  인피니티 워  \n",
       "7  인피니티 워  \n",
       "8  인피니티 워  \n",
       "9  인피니티 워  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('review.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10점' '18' '1987' '1도' '1점' '1점도' '2시간' '2시간이' '2편' '5점' '6점' '7점' '8점'\n",
      " 'cg' 'cg가' 'cg는' 'cg도' 'cg만' 'good' 'of' 'ㅋㅋ' 'ㅋㅋㅋ' 'ㅋㅋㅋㅋ' 'ㅎㅎ' 'ㅎㅎㅎ'\n",
      " 'ㅜㅜ' 'ㅠㅠ' 'ㅠㅠㅠ' 'ㅡㅡ' '가는' '가는줄' '가면' '가서' '가슴' '가슴아픈' '가슴이' '가장' '가족'\n",
      " '가족과' '가족들과' '가족의' '가족이' '가지고' '간만에' '갈수록' '감독' '감독님' '감독은' '감독의' '감독이'\n",
      " '감동' '감동과' '감동도' '감동은' '감동을' '감동이' '감동입니다' '감동적' '감동적이고' '감동적인' '감사드립니다'\n",
      " '감사합니다' '감정이' '갑자기' '갔는데' '갔다가' '강철비' '강추' '강추합니다' '같고' '같네요' '같다' '같습니다'\n",
      " '같아' '같아요' '같은' '같은데' '같음' '같이' '개연성' '개연성이' '개인적으로' '거의' '겁나' '것도' '것은'\n",
      " '것을' '것이' '것이다' '겨울왕국' '결국' '결말' '결말이' '계속' '고맙습니다' '곤지암' '공포' '공포를'\n",
      " '공포영화' '관객']\n"
     ]
    }
   ],
   "source": [
    "# review 항목으로 카운트 벡터 생성 \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "daum_cv = CountVectorizer(max_features=1000)\n",
    "\n",
    "daum_DTM = daum_cv.fit_transform(df.review) #review를 이용하여 count vector를 학습하고, 변환\n",
    "print(daum_cv.get_feature_names_out()[:100]) # count vector에 사용된 feature 이름을 반환]\n",
    "\n",
    "# cg 가 들어간 단어들(cg, cg가, cg는, cg도, cg 만)- 별도의 단어로 분류됨 \n",
    "# 감동이 들어간 단어들(감동, 감동과, 감동도, 감동은)\n",
    "# 이런 단어들은 의미적으로 같은 단어로 취급해야 함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#전체 형태소 결과: ['몰입', '할수밖에', '없다', '.', '어렵게', '생각', '할', '필요없다', '.', '내', '가', '전투', '에', '참여', '한', '듯', '손', '에', '땀', '이남', '.']\n",
      "#명사만 추출: ['몰입', '생각', '내', '전투', '참여', '듯', '손', '땀', '이남']\n",
      "#품사 태깅 결과 [('몰입', 'Noun'), ('할수밖에', 'Verb'), ('없다', 'Adjective'), ('.', 'Punctuation'), ('어렵게', 'Adjective'), ('생각', 'Noun'), ('할', 'Verb'), ('필요없다', 'Adjective'), ('.', 'Punctuation'), ('내', 'Noun'), ('가', 'Josa'), ('전투', 'Noun'), ('에', 'Josa'), ('참여', 'Noun'), ('한', 'Determiner'), ('듯', 'Noun'), ('손', 'Noun'), ('에', 'Josa'), ('땀', 'Noun'), ('이남', 'Noun'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt #konlpy에서 Twitter 형태소 분석기를 import\n",
    "twitter_tag = Okt()\n",
    "\n",
    "print('#전체 형태소 결과:', twitter_tag.morphs(df.review[1]))\n",
    "print('#명사만 추출:', twitter_tag.nouns(df.review[1]))\n",
    "print('#품사 태깅 결과', twitter_tag.pos(df.review[1]))\n",
    "# 품사 태깅 결과로 Josa, Punctuation 을 제외함 \n",
    "# 명사, 동사, 형용사만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나만의 토크나이저 결과: ['몰입', '할수밖에', '없다', '어렵게', '생각', '할', '필요없다', '내', '전투', '참여', '듯', '손', '땀', '이남']\n"
     ]
    }
   ],
   "source": [
    "def my_tokenizer(doc):\n",
    "    return [token for token, pos in twitter_tag.pos(doc) if pos in ['Noun', 'Verb', 'Adjective']]\n",
    "\n",
    "print(\"나만의 토크나이저 결과:\", my_tokenizer(df.review[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가' '가는' '가는줄' '가면' '가서' '가슴' '가장' '가족' '가족영화' '가지' '가치' '각색' '간' '간다'\n",
      " '간만' '갈' '갈수록' '감' '감독' '감동' '감사' '감사합니다' '감상' '감성' '감정' '감탄' '갑자기' '갔는데'\n",
      " '갔다' '갔다가' '강' '강철' '강추' '같고' '같네요' '같다' '같습니다' '같아' '같아요' '같은' '같은데'\n",
      " '같음' '개' '개그' '개봉' '개연' '개인' '거' '거기' '거리' '거의' '걱정' '건' '건가' '건지' '걸'\n",
      " '겁니다' '것' '게' '겨울왕국' '결론' '결말' '경찰' '경험' '계속' '고' '고맙습니다' '고민' '고생' '곤지암'\n",
      " '곳' '공감' '공포' '공포영화' '과' '과거' '관' '관객' '관객수' '관람' '광주' '괜찮은' '교훈' '구성'\n",
      " '국내' '국민' '군인' '군함도' '굿' '권선' '귀신' '그' '그것' '그게' '그날' '그냥' '그닥' '그대로'\n",
      " '그때' '그래픽']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#토크나이저와 특성의 최대개수를 지정\n",
    "daum_cv = CountVectorizer(max_features=1000, tokenizer=my_tokenizer)\n",
    "#명사만 추출하고 싶은 경우에는 tokenizer에 'twitter_tag.nouns'를 바로 지정해도 됨\n",
    "\n",
    "daum_DTM = daum_cv.fit_transform(df.review) #review를 이용하여 count vector를 학습하고, 변환\n",
    "print(daum_cv.get_feature_names_out()[:100]) # count vector에 사용된 feature 이름을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<14725x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 110800 stored elements in Compressed Sparse Row format>\n",
      "0.007524617996604414\n"
     ]
    }
   ],
   "source": [
    "print(repr(daum_DTM))\n",
    "print(110800/(14725*1000)) # 희소 행렬, 값이 있는 비율 0.75% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내 : 1, 듯 : 1, 몰입 : 1, 생각 : 1, 손 : 1, 없다 : 1, 할 : 1, "
     ]
    }
   ],
   "source": [
    "# 둘째 리뷰에 대해 사용된 단어와 개수 출력 \n",
    "# max_feature: 1,000으로 제한해 빈도가 낮은 단어들 제외 \n",
    "for word, count in zip(daum_cv.get_feature_names_out(), daum_DTM[1].toarray()[0]):\n",
    "    if count > 0:\n",
    "        print(word, ':', count, end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 카운트 벡터의 활용\n",
    "* 카운트 벡터, 문서의 특성 표현 \n",
    "* 문서 유사도를 측정하는데에도 사용됨 \n",
    "* 유사도 측정: 주어진 문서와 가장 유사한 문서를 말뭉치에서 검색 \n",
    "\n",
    "### 코사인 유사도(Cosine similarity)\n",
    "* 두 벡터가 이루는 각도의 코사인 값 \n",
    "* 벡터의 크기는 중요하지 않고, 벡터의 방향성만 비교 \n",
    "* 두 벡터가 가장 가까우면(각도 일치): 유사도 1 \n",
    "* 가장 먼 경우 (각도가 가장 크면): 유사도 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import numpy as np\n",
    "x = np.arange(0,2*np.pi,0.1)   # start,stop,step \n",
    "y = np.cos(x)\n",
    "#print(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#대상 특성 행렬의 크기: (1, 1000)\n",
      "#유사도 계산 행렬의 크기: (1, 2000)\n",
      "#유사도 계산결과를 역순으로 정렬: [0.8367205630128807, 0.43817531290756406, 0.4080451370075411, 0.40727044884302327, 0.4060219836225451, 0.3999621981759778, 0.39965783997760135, 0.39566661804603703, 0.3945302295079114, 0.3911637170821695]\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------\n",
    "# cosine_similarity \n",
    "    # 다수 벡터, 다수 벡터간의 유사도 한번에 계산 (벡터 하나씩 유사도 계산 X)\n",
    "    # 결과로 행렬을 반환 \n",
    "    # m개 벡터, n개 벡터간의 유사도 한꺼번 계산 (m,n)의 2차원 행렬 \n",
    "\n",
    "# 아래 예시: 대상 문서 하나, 이를 전체 리뷰와 비교 (1,2000) 행렬로 반환\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "start = len(reviews[0]) // 2 #첫째 리뷰의 문자수를 확인하고 뒤 절반을 가져오기 위해 중심점을 찾음\n",
    "source = reviews[0][-start:] #중심점으로부터 뒤 절반을 가져와서 비교할 문서를 생성\n",
    "\n",
    "source_cv = cv.transform([source]) #코사인 유사도는 카운트 벡터에 대해 계산하므로 벡터로 변환\n",
    "#transform은 반드시 리스트나 행렬 형태의 입력을 요구하므로 리스트로 만들어서 입력\n",
    "\n",
    "print(\"#대상 특성 행렬의 크기:\", source_cv.shape) #행렬의 크기를 확인, 문서가 하나이므로 (1, 1000)\n",
    "\n",
    "sim_result = cosine_similarity(source_cv, reviews_cv) #변환된 count vector와 기존 값들과의 similarity 계산\n",
    "\n",
    "print(\"#유사도 계산 행렬의 크기:\", sim_result.shape)\n",
    "print(\"#유사도 계산결과를 역순으로 정렬:\", sorted(sim_result[0], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#가장 유사한 리뷰의 인덱스: 0\n"
     ]
    }
   ],
   "source": [
    "# np.argmax  - 가장 높은 항목의 인덱스(index) 가져옴 \n",
    "\n",
    "import numpy as np\n",
    "print('#가장 유사한 리뷰의 인덱스:', np.argmax(sim_result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#가장 유사한 리뷰부터 정렬한 인덱스: [   0 1110 1570  687  628  112 1712 1393  524 1740]\n"
     ]
    }
   ],
   "source": [
    "# np.argsort - 유사도가 가장 가까운 것들부터 인덱스를 순서대로 \n",
    "\n",
    "print('#가장 유사한 리뷰부터 정렬한 인덱스:', (-sim_result[0]).argsort()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 TF-IDF로 성능을 높여보자\n",
    "* 카운트 벡터의 단점 \n",
    "    * 빈도가 높을수록 중요한 단어로 취급받음 \n",
    "    * but, 모든 문서에 다 들어있는 단어는 별로 중요하지 않음 (ex. a, the 등)\n",
    "    * 단어가 더 많은 문서에 나타날 수록, 그 단어는 별로 중요하지 않은 경우가 더 많음 \n",
    "\n",
    "* TF-IDF(Term Frequency - Inverse Document Frequency)\n",
    "    * 단어빈도 - 역문서빈도 \n",
    "    * 카운트 대신 단어의 빈도에 그 단어가 출현한 문서 수의 역수를 곱합 \n",
    "    * 단어의 빈도 / 그 단어가 나타난 문서의 수 \n",
    "    * 단어가 나타난 문서의 수가 클수록 단어 중요도 낮아짐 \n",
    "\n",
    "* TfidfVectorizer\n",
    "    * 텍스트로부터 바로 TF-IDF 행렬 생성 \n",
    "* TfidfTransformer\n",
    "    * 카운트 벡터로부터 변환함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#shape of tfidf matrix: (2000, 1000)\n",
      "#20 count score of the first review: [6 3 6 3 3 0 2 0 1 0 1 3 2 2 1 0 1 2 3 5]\n",
      "#20 tfidf score of the first review: [0.13556199 0.06700076 0.14998642 0.0772298  0.08608998 0.\n",
      " 0.0609124  0.         0.03126552 0.         0.03242315 0.09567082\n",
      " 0.06575035 0.06518293 0.03225625 0.         0.0345017  0.06863314\n",
      " 0.10042383 0.16727495]\n"
     ]
    }
   ],
   "source": [
    "# TfidfTransformer, 카운트 벡터 사용한 경우 \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "transformer\n",
    "\n",
    "reviews_tfidf = transformer.fit_transform(reviews_cv)\n",
    "print('#shape of tfidf matrix:', reviews_tfidf.shape) #TF-IDF 행렬의 모양과 카운트 행렬의 모양이 일치하는 것을 확인\n",
    "\n",
    "#첫 리뷰의 카운트 벡터 중 앞 20개 값 출력\n",
    "print('#20 count score of the first review:', reviews_cv[0].toarray()[0][:20]) \n",
    "#첫 리뷰의 TF-IDF 벡터 중 앞 20개 값 출력\n",
    "print('#20 tfidf score of the first review:', reviews_tfidf[0].toarray()[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#가장 유사한 리뷰의 인덱스: 0\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer, 카운트 벡터 거치지 않고 처음부터 TF-IDF 행렬 생성한 경우 \n",
    "# TF-IDF에 대해 코사인 유사도 구함 (여기선, word_features를 사용)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(vocabulary=word_features) # word_features[0:5] ['film', 'one', 'movie', 'like', 'even']\n",
    "reviews_tf = tf.fit_transform(reviews) # review 원문 - tfidf 값으로 변환 (백터)\n",
    "\n",
    "#코사인 유사도는 카운트 벡터에 대해 계산하므로 벡터로 변환\n",
    "# source = reviews[0][-start:] \n",
    "# 중심점으로부터 뒤 절반을 가져와서 비교할 문서를 생성\n",
    "#transform은 반드시 리스트나 행렬 형태의 입력을 요구하므로 리스트로 만들어서 입력\n",
    "source_tf = tf.transform([source]) \n",
    "\n",
    "#변환된 count vector와 기존 값들과의 similarity 계산\n",
    "sim_result_tf = cosine_similarity(source_tf, reviews_tf) \n",
    "\n",
    "print('#가장 유사한 리뷰의 인덱스:', np.argmax(sim_result_tf[0]))\n",
    "\n",
    "vocabulary = word_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#카운트 벡터에 대해 가장 유사한 리뷰부터 정렬한 인덱스: [   0 1110 1570  687  628  112 1712 1393  524 1740]\n",
      "#TF-IDF 벡터에 대해 가장 유사한 리뷰부터 정렬한 인덱스: [   0 1110 1393 1570  645  323 1143  628 1676 1391]\n"
     ]
    }
   ],
   "source": [
    "print('#카운트 벡터에 대해 가장 유사한 리뷰부터 정렬한 인덱스:', (-sim_result[0]).argsort()[:10])\n",
    "print('#TF-IDF 벡터에 대해 가장 유사한 리뷰부터 정렬한 인덱스:', (-sim_result_tf[0]).argsort()[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
